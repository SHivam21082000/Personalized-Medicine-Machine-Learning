{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                           <p>           MACHINE LEARNING SPECIAL ASSIGNMENT    </p>\n",
    "\n",
    "#                          Personalized Medicine :  Redifining Cancer Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e4f060ddaeb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "# Loading all required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training_variants. Its a comma seperated file\n",
    "data_variants = pd.read_csv('training_variants')\n",
    "# Loading training_text dataset. This is seperated by ||\n",
    "data_text =pd.read_csv(\"training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li><b> ID : </b>row id used to link the mutation to the clinical evidence</li>\n",
    "<li><b> Gene : </b>the gene where this genetic mutation is located</li> \n",
    "<li><b> Variation :</b> the aminoacid change for this mutations </li>\n",
    "<li><b> Class :</b> class value 1-9, this genetic mutation has been classified on</li>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset looks like the following:\n",
    "\n",
    " * data_variants (ID, Gene, Variations, Class)\n",
    " * data_text(ID, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variants.Class.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have more than one classes here to predict, it is a multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collects words like a,in,the from nltk library which we don't want\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_text_preprocess(total_text, ind, col):\n",
    "   \n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        # replacing all special char with space\n",
    "        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', str(total_text))\n",
    "        # replacing multiple spaces with single space\n",
    "        total_text = re.sub('\\s+',' ', str(total_text))\n",
    "        # bring whole text to same lower-case scale.\n",
    "        total_text = total_text.lower()\n",
    "        \n",
    "        for word in total_text.split():\n",
    "        # if the word is a not a stop word then retain that word from text\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        \n",
    "        data_text[col][ind] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_text.iterrows():\n",
    "    if type(row['TEXT']) is str:\n",
    "        data_text_preprocess(row['TEXT'], index, 'TEXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging both gene_variations and text data based on ID\n",
    "\n",
    "result = pd.merge(data_variants, data_text,on='ID', how='left')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks for missing values\n",
    "result[result.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fills the missing vales my concatenation of gene and variation\n",
    "result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating Training, Test and Validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to split the gene and variation by _\n",
    "y_true = result['Class'].values\n",
    "result.Gene      = result.Gene.str.replace('\\s+', '_')\n",
    "result.Variation = result.Variation.str.replace('\\s+', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can now start our split process in train, test and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test set \n",
    "X_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n",
    "# split the train data now into train validation and cross validation\n",
    "train_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of data points in train data:', train_df.shape[0])\n",
    "print('Number of data points in test data:', test_df.shape[0])\n",
    "print('Number of data points in cross validation data:', cv_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see the distribution of data in train, test and validation set.\n",
    "train_class_distribution = train_df['Class'].value_counts().sort_index()\n",
    "test_class_distribution = test_df['Class'].value_counts().sort_index()\n",
    "cv_class_distribution = cv_df['Class'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*** Visualizing for train class distrubution***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colors = 'rgbkymc'\n",
    "train_class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel(' Number of Data points per Class')\n",
    "plt.title('Distribution of yi in train data')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distrubution in the form of percentage\n",
    "sorted_yi = np.argsort(-train_class_distribution.values)\n",
    "for i in sorted_yi:\n",
    "    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/train_df.shape[0]*100), 3), '%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colors = 'rgbkymc'\n",
    "test_class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Data points per Class')\n",
    "plt.title('Distribution of yi in test data')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_yi = np.argsort(-test_class_distribution.values)\n",
    "for i in sorted_yi:\n",
    "    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/test_df.shape[0]*100), 3), '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colors = 'rgbkymc'\n",
    "cv_class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Data points per Class')\n",
    "plt.title('Distribution of yi in cross validation data')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_yi = np.argsort(-train_class_distribution.values)\n",
    "for i in sorted_yi:\n",
    "    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]/cv_df.shape[0]*100), 3), '%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_len = train_df.shape[0]\n",
    "test_data_len = test_df.shape[0]\n",
    "cv_data_len = cv_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set error\n",
    "# we create a output array that has exactly same size as the Training data\n",
    "train_predicted_y = np.zeros((train_data_len,9))\n",
    "for i in range(train_data_len):\n",
    "    rand_probs = np.random.rand(1,9)\n",
    "    train_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "print(\"Log loss on Training Data using Random Model\",log_loss(y_train,train_predicted_y, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV set error.\n",
    "# we create a output array that has exactly same size as the CV data\n",
    "cv_predicted_y = np.zeros((cv_data_len,9))\n",
    "for i in range(cv_data_len):\n",
    "    rand_probs = np.random.rand(1,9)\n",
    "    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "print(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Set error.\n",
    "#we create a output array that has exactly same as the test data\n",
    "test_predicted_y = np.zeros((test_data_len,9))\n",
    "for i in range(test_data_len):\n",
    "    rand_probs = np.random.rand(1,9)\n",
    "    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "print(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the index of max probablity\n",
    "predicted_y =np.argmax(test_predicted_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the output. these will be 665 values present in test dataset\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the index starts from 0 in array, we have added 1 to it in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = predicted_y + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_test, predicted_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,2,3,4,5,6,7,8,9]\n",
    "plt.figure(figsize=(20,7))\n",
    "sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Original Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B =(C/C.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Original Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =(((C.T)/(C.sum(axis=1))).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Original Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Gene Column\n",
    "\n",
    "Now we will look at each independent column to make sure its relavent for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genes = train_df['Gene'].value_counts()\n",
    "print('Number of Unique Genes :', unique_genes.shape[0])\n",
    "# the top 10 genes that occured most\n",
    "print(unique_genes.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the number of unique values present in gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genes.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a need to convert these categorical variable to appropirate format which our machine learning algorithm will be able to take as an input.\n",
    "\n",
    "There are two methods:\n",
    "\n",
    "<ol><li>\n",
    "     One-hot encoding </li>\n",
    "    <li> Response Encoding </li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of Gene feature.\n",
    "gene_vectorizer = CountVectorizer()\n",
    "train_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\n",
    "test_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\n",
    "cv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding will always return higher number of column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene_feature_onehotCoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names after one-hot encoding for Gene column\n",
    "gene_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets also create Response encoding columns for Gene column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gv_fea_dict(alpha, feature, df):\n",
    "   \n",
    "    value_count = train_df[feature].value_counts()\n",
    "    gv_dict = dict()\n",
    "    for i, denominator in value_count.items():\n",
    "     \n",
    "        vec = []\n",
    "        \n",
    "        for k in range(1,10):\n",
    "\n",
    "            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n",
    "            \n",
    "            vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n",
    "\n",
    "        gv_dict[i]=vec\n",
    "    return gv_dict\n",
    "\n",
    "\n",
    "def get_gv_feature(alpha, feature, df):\n",
    "\n",
    "    gv_dict = get_gv_fea_dict(alpha, feature, df)\n",
    "    # value_count is similar in get_gv_fea_dict\n",
    "    value_count = train_df[feature].value_counts()\n",
    "    \n",
    "    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n",
    "    gv_fea = []\n",
    "    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n",
    "    # if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n",
    "    for index, row in df.iterrows():\n",
    "        if row[feature] in dict(value_count).keys():\n",
    "            gv_fea.append(gv_dict[row[feature]])\n",
    "        else:\n",
    "            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
    "\n",
    "    return gv_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response-coding of the Gene feature\n",
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train gene feature\n",
    "train_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df))\n",
    "# test gene feature\n",
    "test_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df))\n",
    "# cross validation gene feature\n",
    "cv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must be having 9 columns for Gene column after response encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene_feature_responseCoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a hyperparemeter for SGD classifier.\n",
    "alpha = [10 ** x for x in range(-5, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using SGD classifier\n",
    "# We will also be using Calibrated Classifier to get the result into probablity format to be used for log loss\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_gene_feature_onehotCoding, y_train)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n",
    "    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the same to check the best Alpha value\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use best alpha value as we can see from above graph and compute log loss\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_gene_feature_onehotCoding, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check how many values are overlapping between train, test or between CV and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coverage=test_df[test_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n",
    "cv_coverage=cv_df[cv_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\n",
    "print('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Variation column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation is also a categorical variable so we have to deal in same way like we have done for ***Gene*** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variations = train_df['Variation'].value_counts()\n",
    "print('Number of Unique Variations :', unique_variations.shape[0])\n",
    "# the top 10 variations that occured most\n",
    "print(unique_variations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the variation column using one hot encoder column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of variation feature.\n",
    "variation_vectorizer = CountVectorizer()\n",
    "train_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\n",
    "test_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\n",
    "cv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the shape of one hot encoder column for variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_variation_feature_onehotCoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for variation column and generate response encoding for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is used for laplace smoothing\n",
    "alpha = 1\n",
    "# train gene feature\n",
    "train_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df))\n",
    "# test gene feature\n",
    "test_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df))\n",
    "# cross validation gene feature\n",
    "cv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the shape of this response encoding result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_variation_feature_responseCoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets again build the model with only column name of ***variation*** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a hyperparemeter for SGD classifier.\n",
    "alpha = [10 ** x for x in range(-5, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using SGD classifier\n",
    "# We will also be using Calibrated Classifier to get the result into probablity format t be used for log loss\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_variation_feature_onehotCoding, y_train)\n",
    "    \n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n",
    "    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n",
    "    \n",
    "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the same to check the best Alpha value\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_variation_feature_onehotCoding, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\n",
    "cv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\n",
    "print('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dictionary_paddle(cls_text):\n",
    "    dictionary = defaultdict(int)\n",
    "    for index, row in cls_text.iterrows():\n",
    "        for word in row['TEXT'].split():\n",
    "            dictionary[word] +=1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_text_responsecoding(df):\n",
    "    text_feature_responseCoding = np.zeros((df.shape[0],9))\n",
    "    for i in range(0,9):\n",
    "        row_index = 0\n",
    "        for index, row in df.iterrows():\n",
    "            sum_prob = 0\n",
    "            for word in row['TEXT'].split():\n",
    "                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n",
    "            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n",
    "            row_index += 1\n",
    "    return text_feature_responseCoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a CountVectorizer with all the words that occured minimum 3 times in train data\n",
    "text_vectorizer = CountVectorizer(min_df=3)\n",
    "train_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n",
    "# getting all the feature names (words)\n",
    "train_text_features= text_vectorizer.get_feature_names()\n",
    "\n",
    "# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\n",
    "train_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n",
    "\n",
    "# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\n",
    "text_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n",
    "\n",
    "\n",
    "print(\"Total number of unique words in train data :\", len(train_text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "# dict_list =[] contains 9 dictoinaries each corresponds to a class\n",
    "for i in range(1,10):\n",
    "    cls_text = train_df[train_df['Class']==i]\n",
    "    # build a word dict based on the words in that class\n",
    "    dict_list.append(extract_dictionary_paddle(cls_text))\n",
    "    # append it to dict_list\n",
    "\n",
    "# dict_list[i] is build on i'th  class text data\n",
    "# total_dict is buid on whole training text data\n",
    "total_dict = extract_dictionary_paddle(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response coding of text features\n",
    "train_text_feature_responseCoding  = get_text_responsecoding(train_df)\n",
    "test_text_feature_responseCoding  = get_text_responsecoding(test_df)\n",
    "cv_text_feature_responseCoding  = get_text_responsecoding(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert each row values such that they sum to 1  \n",
    "train_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\n",
    "test_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n",
    "cv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To normalize every feature\n",
    "train_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "# we use the same vectorizer that was trained on train data\n",
    "test_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n",
    "# To normalize every feature\n",
    "test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "# we use the same vectorizer that was trained on train data\n",
    "cv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n",
    "# To normalize every feature\n",
    "cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\n",
    "sorted_text_occur = np.array(list(sorted_text_fea_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words for a given frequency.\n",
    "print(Counter(sorted_text_occur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the model with only ***text*** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "    \n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the overlap of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersec_text(df):\n",
    "    df_text_vec = CountVectorizer(min_df=3)\n",
    "    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n",
    "    df_text_features = df_text_vec.get_feature_names()\n",
    "\n",
    "    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n",
    "    df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n",
    "    len1 = len(set(df_text_features))\n",
    "    len2 = len(set(train_text_features) & set(df_text_features))\n",
    "    return len1,len2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1,len2 = get_intersec_text(test_df)\n",
    "print(np.round((len2/len1)*100, 3), \"% of word of test data appeared in train data\")\n",
    "len1,len2 = get_intersec_text(cv_df)\n",
    "print(np.round((len2/len1)*100, 3), \"% of word of Cross Validation appeared in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, all 3 columns are going to be important.\n",
    "\n",
    "## Data prepration for Machine Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create few functions which we will be using later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n",
    "    clf.fit(train_x, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(test_x)\n",
    "    return log_loss(test_y, sig_clf_probs, eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the confusion matrices given y_i, y_i_hat.\n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    \n",
    "    #confusion matrix\n",
    "    B =(C/C.sum(axis=0)) \n",
    "    labels = [1,2,3,4,5,6,7,8,9]\n",
    "    # representing A in heatmap format\n",
    "    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "\n",
    "    #precision matrix\n",
    "    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    \n",
    "    # representing B in heatmap format\n",
    "    #recall matrix\n",
    "    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n",
    "    clf.fit(train_x, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x, train_y)\n",
    "    pred_y = sig_clf.predict(test_x)\n",
    "\n",
    "    # for calculating log_loss we will provide the array of probabilities belongs to each class\n",
    "    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n",
    "    # calculating the number of data points that are misclassified\n",
    "    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n",
    "    plot_confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all 3 features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging gene, variance and text features\n",
    "\n",
    "# building train, test and cross validation data sets\n",
    "# a = [[1, 2], \n",
    "#      [3, 4]]\n",
    "# b = [[4, 5], \n",
    "#      [6, 7]]\n",
    "# hstack(a, b) = [[1, 2, 4, 5],\n",
    "#                [ 3, 4, 6, 7]]\n",
    "\n",
    "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
    "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
    "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
    "\n",
    "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\n",
    "train_y = np.array(list(train_df['Class']))\n",
    "\n",
    "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
    "test_y = np.array(list(test_df['Class']))\n",
    "\n",
    "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n",
    "cv_y = np.array(list(cv_df['Class']))\n",
    "\n",
    "\n",
    "train_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\n",
    "test_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\n",
    "cv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n",
    "\n",
    "train_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\n",
    "test_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\n",
    "cv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One hot encoding features :\")\n",
    "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
    "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
    "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Response encoding features :\")\n",
    "print(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\n",
    "print(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\n",
    "print(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start the first model which is most suitable when we have lot of text column data. So, we will start with Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = MultinomialNB(alpha=i)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.log10(alpha), cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.xticks(np.log10(alpha))\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our Naive Bayes model with best found value of alpha on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "# to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
    "print(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])\n",
    "plot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Naive Bayes not performing very badly but we need to look at other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [5, 11, 15, 21, 31, 41, 51, 99]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = KNeighborsClassifier(n_neighbors=i)\n",
    "    clf.fit(train_x_responseCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_responseCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
    "clf.fit(train_x_responseCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_responseCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_responseCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_responseCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_responseCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to test it on testing dataset with our best alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
    "predict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to test it on testing data using best alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 3)]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for C =\", i)\n",
    "#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n",
    "    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model with best alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\n",
    "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
